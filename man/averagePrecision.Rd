% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Evaluation.R
\name{averagePrecision}
\alias{averagePrecision}
\title{Average Precision performance metric}
\usage{
averagePrecision(trueLabel, scores, adjusted = FALSE)
}
\arguments{
\item{trueLabel}{Binary vector with known Labels from training data, 1
indicating an outlier and 0 a regular object}

\item{scores}{Vector of score values}

\item{adjusted}{adjusted if TRUE, the average precision is adjusted for
chance to allow comparison between data sets that consist of different
proportions of outliers.}
}
\value{
Numeric Average Precision value
}
\description{
See referenced paper by Campos et al. for explanation.
}
\references{
Guilherme O. Campos, Arthur Zimek, Joerg Sander, Ricardo J. G. B.
  Campello, Barbora Micenkova, Erich Schubert, Ira Assent, Michael E. Houle.
  2016. 'On the Evaluation of Unsupervised Outlier Detection: Measures,
  Datasets, and an Empirical Study.' Data Mining and Knowledge Discovery,
  January, p. 1 to 37.
}
